#!/usr/bin/env python3
"""
Test de performance para identificar por qu√© OpenAI tarda tanto
"""

import os
import json
import time
from openai import OpenAI
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

api_key = os.getenv("CHAT_GPT_MODEL") or os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

print("üîç AN√ÅLISIS DE PERFORMANCE DE OPENAI")
print("=" * 60)

def test_scenario(name, system_prompt, user_prompt, **kwargs):
    """Prueba un escenario espec√≠fico y mide el tiempo"""
    print(f"\nüìä TEST: {name}")
    print(f"   System prompt length: {len(system_prompt)} chars")
    print(f"   Config: {kwargs}")

    start = time.time()
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            **kwargs
        )
        elapsed = time.time() - start

        content = response.choices[0].message.content
        tokens = response.usage.completion_tokens

        print(f"   ‚úÖ Tiempo: {elapsed:.2f}s")
        print(f"   ‚úÖ Tokens: {tokens}")
        print(f"   ‚úÖ Chars: {len(content)}")
        print(f"   ‚úÖ Tokens/segundo: {tokens/elapsed:.1f}")

        # Verificar si es JSON v√°lido
        if "json" in name.lower():
            try:
                json.loads(content)
                print(f"   ‚úÖ JSON v√°lido")
            except:
                print(f"   ‚ùå JSON inv√°lido")

        return elapsed, tokens

    except Exception as e:
        elapsed = time.time() - start
        print(f"   ‚ùå Error despu√©s de {elapsed:.2f}s: {e}")
        return elapsed, 0

# Prompt de usuario com√∫n para todos los tests
user_prompt = """D√≠a 1 (Lunes)
2000cal
5 comidas
Meta: cut"""

print("\nüß™ ESCENARIO 1: Prompt actual problem√°tico")
test_scenario(
    "Prompt actual con JSON format",
    """SOLO JSON. 5 comidas/d√≠a. Max 2 ingredientes.
{"days":[{"day_number":1,"day_name":"D√≠a","meals":[{"name":"nombre","meal_type":"breakfast|snack|lunch|dinner","calories":400,"protein":30,"carbs":45,"fat":10,"ingredients":[{"name":"ing","quantity":100,"unit":"g"}],"instructions":"prep"}]}]}""",
    user_prompt,
    temperature=0.2,
    max_tokens=800,
    response_format={"type": "json_object"}
)

print("\nüß™ ESCENARIO 2: Sin response_format JSON")
test_scenario(
    "Sin JSON format enforcement",
    """SOLO JSON. 5 comidas/d√≠a. Max 2 ingredientes.
{"days":[{"day_number":1,"day_name":"D√≠a","meals":[{"name":"nombre","meal_type":"breakfast|snack|lunch|dinner","calories":400,"protein":30,"carbs":45,"fat":10,"ingredients":[{"name":"ing","quantity":100,"unit":"g"}],"instructions":"prep"}]}]}""",
    user_prompt,
    temperature=0.2,
    max_tokens=800
    # Sin response_format
)

print("\nüß™ ESCENARIO 3: Prompt m√°s claro y estructurado")
test_scenario(
    "Prompt mejorado con JSON format",
    """Genera un plan nutricional en formato JSON.

Estructura requerida:
- 1 d√≠a con 5 comidas
- Cada comida: nombre, tipo (breakfast/snack/lunch/dinner), calor√≠as, prote√≠na, carbohidratos, grasa
- M√°ximo 2 ingredientes por comida
- Instrucciones breves

Responde SOLO con JSON v√°lido.""",
    user_prompt,
    temperature=0.2,
    max_tokens=800,
    response_format={"type": "json_object"}
)

print("\nüß™ ESCENARIO 4: Prompt simple sin ejemplo")
test_scenario(
    "Prompt minimalista",
    "Genera un plan nutricional de 1 d√≠a con 5 comidas en formato JSON. Solo JSON v√°lido.",
    user_prompt,
    temperature=0.2,
    max_tokens=800,
    response_format={"type": "json_object"}
)

print("\nüß™ ESCENARIO 5: Sin ninguna restricci√≥n")
test_scenario(
    "Sin restricciones",
    "Eres un nutricionista. Genera un plan de alimentaci√≥n.",
    user_prompt,
    temperature=0.2,
    max_tokens=800
)

print("\nüß™ ESCENARIO 6: Temperatura 0 (m√°s determin√≠stico)")
test_scenario(
    "Temperatura 0",
    """Genera un plan nutricional en formato JSON.
Solo JSON v√°lido. 1 d√≠a, 5 comidas.""",
    user_prompt,
    temperature=0,
    max_tokens=800,
    response_format={"type": "json_object"}
)

print("\n" + "=" * 60)
print("üìä CONCLUSIONES:")
print("- response_format JSON puede agregar overhead")
print("- Prompts confusos o con ejemplos mal formateados causan demoras")
print("- El modelo puede estar 'pensando' m√°s cuando el prompt es ambiguo")
print("=" * 60)